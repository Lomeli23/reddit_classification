{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37be1ada-88d0-4e06-bd6c-614db58aa4c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reddit Classification: \n",
    "### Using NLP to predict, which subreddit a particular post or comment came from.\n",
    "By: Nick Lomeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b4b4b-901c-4be2-b2da-1981df4dd35a",
   "metadata": {},
   "source": [
    "----\n",
    "## Part 1: Data Collection, Cleaning, and Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b926eb-1fa0-4fe2-8407-d134747cbaf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493acfe-830a-422c-ad83-8cb3abe14572",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd18d9-7fec-4a38-bef9-2940579a380d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Obtain auth / access token from the reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ffe5c7-1c64-4f4c-9ff3-263169904bbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_id = 'MS6yOenwdShJwO324Nf_cA'\n",
    "secret_id = 'TEwJl2TLEl3sa5B76Zmm59FUggAIRA'\n",
    "\n",
    "auth = requests.auth.HTTPBasicAuth(client_id, secret_id)\n",
    "\n",
    "with open ('../../Notes/Breakfast_Hours/pw.txt', 'r') as f:\n",
    "    pw = f.read()\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': 'Lower_Lemon9227',\n",
    "    'password': pw\n",
    "}\n",
    "\n",
    "headers = {'User-Agent': 'MyAPI/0.02'}\n",
    "\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth = auth,\n",
    "                    data = data,\n",
    "                    headers = headers\n",
    "                   )\n",
    "\n",
    "access_token = res.json()['access_token']\n",
    "\n",
    "headers['authorization'] = f'bearer {access_token}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d67b2-fd93-40a1-818e-feaa4a29e05e",
   "metadata": {},
   "source": [
    "### Create a function that:\n",
    "- Uses the Reddit API to web-scrape data from a specified subreddit.\n",
    "- Filters posts from the subreddit to require that it has a time stamp, title, text, comments, and a score before including any in the web-scrape.\n",
    "- Converts the data into a DataFrame and cleans it for readability.\n",
    "- Prints the unique id of the last post from each web-scraping iteration. \n",
    "     - The last comment's unique id will later be passed to a subsequent function that will use it as a starting point reference for the next web-scraping iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0a0068-9996-427b-bd36-d563b616db4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_initial(subreddit, get_type):\n",
    "    \n",
    "    # URL\n",
    "    url = f'https://oauth.reddit.com/r/{subreddit}/{get_type}'\n",
    "    \n",
    "    # Add params\n",
    "    params = {'filter': ['created_utc', subreddit, 'title', 'selftext', 'num_comments', 'score']\n",
    "             }\n",
    "    \n",
    "    # Get the data\n",
    "    res = requests.get(url, \n",
    "                      headers = headers\n",
    "                 )\n",
    "    \n",
    "    # Convert the request into a list of dict objects\n",
    "    data = res.json()\n",
    "    \n",
    "    # Go deeper to clean up data and create desired columns based on available keys\n",
    "    n = len(data['data']['children'])\n",
    "    created_utc_col = [data['data']['children'][i]['data']['created_utc'] for i in range(1, n)]\n",
    "    subreddit_col = [data['data']['children'][i]['data']['subreddit'] for i in range(1, n)]\n",
    "    title_col = [data['data']['children'][i]['data']['title'] for i in range(1, n)]\n",
    "    selftext_col = [data['data']['children'][i]['data']['selftext'] for i in range(1, n)]\n",
    "    num_comments_col = [data['data']['children'][i]['data']['num_comments'] for i in range(1, n)]\n",
    "    score_col = [data['data']['children'][i]['data']['score'] for i in range(1, n)]\n",
    "\n",
    "    #Turn into a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'created_utc': created_utc_col,\n",
    "        'subreddit': subreddit_col,\n",
    "        'title': title_col,\n",
    "        'text': selftext_col,\n",
    "        'num_comments': num_comments_col,\n",
    "        'score': score_col\n",
    "    })\n",
    "    \n",
    "    comment_id = f\"{res.json()['data']['children'][-1]['kind']}_{res.json()['data']['children'][-1]['data']['id']}\"\n",
    "\n",
    "    print(comment_id)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8765e027-8923-4f44-b2e2-38b6e5be7400",
   "metadata": {},
   "source": [
    "### Create a second function that:\n",
    "- Accepts a subreddit, subreddit post type, and unique comment id as an argument\n",
    "    - Uses the unique comment id passed as a place to start web-scraping posts/comments after.\n",
    "        - the unique comment id passed should be the last id printed from the previous iteration.\n",
    "    - Completes the same steps as in the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b71b40c-82ef-4c68-a3bd-e31a45f5ee17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_after(subreddit, get_type, last_comment_id):\n",
    "    \n",
    "    # URL\n",
    "    url = f'https://oauth.reddit.com/r/{subreddit}/{get_type}'\n",
    "    \n",
    "    # Add params\n",
    "    params = {'filter': ['created_utc', subreddit, 'title', 'selftext', 'num_comments', 'score'],\n",
    "              'after': last_comment_id\n",
    "             }\n",
    "    \n",
    "    # Get the data\n",
    "    res = requests.get(url, \n",
    "                      headers = headers,\n",
    "                      params = params,\n",
    "                 )\n",
    "    \n",
    "    # Convert the request into a list of dict objects\n",
    "    data = res.json()\n",
    "    \n",
    "    # Go deeper to clean up data and create desired columns based on available keys\n",
    "    n = len(data['data']['children'])\n",
    "    created_utc_col = [data['data']['children'][i]['data']['created_utc'] for i in range(1, n)]\n",
    "    subreddit_col = [data['data']['children'][i]['data']['subreddit'] for i in range(1, n)]\n",
    "    title_col = [data['data']['children'][i]['data']['title'] for i in range(1, n)]\n",
    "    selftext_col = [data['data']['children'][i]['data']['selftext'] for i in range(1, n)]\n",
    "    num_comments_col = [data['data']['children'][i]['data']['num_comments'] for i in range(1, n)]\n",
    "    score_col = [data['data']['children'][i]['data']['score'] for i in range(1, n)]\n",
    "\n",
    "\n",
    "    \n",
    "    #Turn into a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'created_utc': created_utc_col,\n",
    "        'subreddit': subreddit_col,\n",
    "        'title': title_col,\n",
    "        'text': selftext_col,\n",
    "        'num_comments': num_comments_col,\n",
    "        'score': score_col\n",
    "    })\n",
    "    \n",
    "    comment_id = f\"{res.json()['data']['children'][-1]['kind']}_{res.json()['data']['children'][-1]['data']['id']}\"\n",
    "    \n",
    "    print(comment_id) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5505d-be56-4d38-b6ec-0368ff5635ae",
   "metadata": {},
   "source": [
    "## Subreddit 1: **Python**\n",
    "\n",
    "### Web-scrape data from the **python** subreddit and combine all requests into one DataFrame.\n",
    "##### Note that each subreddit has \"hot\", \"new\", and other sections within it. I am most interested in accessing the posts from the \"hot\" and \"new\" sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c26e52d3-70eb-4648-9a42-8afe6cb55ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_160imya\n"
     ]
    }
   ],
   "source": [
    "python_hot_1 = get_data_initial('python', 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376a7a46-2248-48d6-9286-3dfb2a4c0497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_1600evn\n"
     ]
    }
   ],
   "source": [
    "python_hot_2 = get_data_after('python', 'hot', 't3_160apiq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f378cd1-9669-4cbc-a1f8-261a4b040868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15z4wmy\n"
     ]
    }
   ],
   "source": [
    "python_hot_3 = get_data_after('python', 'hot', 't3_1600evn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a71d6f1-5290-45f5-80ad-678d38278ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15x7so4\n"
     ]
    }
   ],
   "source": [
    "python_hot_4 = get_data_after('python', 'hot', 't3_15z4wmy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a52d805e-ec58-43a4-8747-61f089633fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15wywbj\n"
     ]
    }
   ],
   "source": [
    "python_hot_5 = get_data_after('python', 'hot', 't3_15x7so4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a7f434-40d4-4cc9-a2fd-090215b3af67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15uscqf\n"
     ]
    }
   ],
   "source": [
    "python_hot_6 = get_data_after('python', 'hot', 't3_15wywbj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93b565dc-4a2d-487c-a030-bbb54fff5607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15t503x\n"
     ]
    }
   ],
   "source": [
    "python_hot_7 = get_data_after('python', 'hot', 't3_15tz2y7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e0b10e-33a9-4136-a8fa-35a42da5a5fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15s386w\n"
     ]
    }
   ],
   "source": [
    "python_hot_8 = get_data_after('python', 'hot', 't3_15t503x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236c1228-aa32-43fd-a865-9a83c35a0899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15pzlt1\n"
     ]
    }
   ],
   "source": [
    "python_hot_9 = get_data_after('python', 'hot', 't3_15s386w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d988576-44f0-4352-9d96-d8aa8262a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15p34aj\n"
     ]
    }
   ],
   "source": [
    "python_hot_10 = get_data_after('python', 'hot', 't3_15pzlt1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c412edfb-5737-459d-83b8-ce552aded443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15nhegh\n"
     ]
    }
   ],
   "source": [
    "python_hot_11 = get_data_after('python', 'hot', 't3_15p34aj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b938e1c-8200-4efc-9d26-1132991f94de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15mjrnr\n"
     ]
    }
   ],
   "source": [
    "python_hot_12 = get_data_after('python', 'hot', 't3_15nhegh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425d6d78-0696-4848-9c68-085c7edffe29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15le96q\n"
     ]
    }
   ],
   "source": [
    "python_hot_13 = get_data_after('python', 'hot', 't3_15mjrnr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "468aa50a-b541-45c1-97db-b310d995be34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15k4qwj\n"
     ]
    }
   ],
   "source": [
    "python_hot_14 = get_data_after('python', 'hot', 't3_15le96q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64843cd4-8033-4cd9-b55e-5de997202d39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15iy8f0\n"
     ]
    }
   ],
   "source": [
    "python_hot_15 = get_data_after('python', 'hot', 't3_15k4qwj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1499c21f-4343-4813-95b4-4bdb6c4ded65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_161678v\n"
     ]
    }
   ],
   "source": [
    "python_new_1 = get_data_initial('python', 'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c3467a-5391-4caf-9c32-7b1e3b55229e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_1600evn\n"
     ]
    }
   ],
   "source": [
    "python_new_2 = get_data_after('python', 'new', 't3_161481e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5acf9977-02c9-4489-b51d-866c6660263d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15yu4if\n"
     ]
    }
   ],
   "source": [
    "python_new_3 = get_data_after('python', 'new', 't3_1600evn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2f08522-6fdf-418e-b989-3adb38b33b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15xjs95\n"
     ]
    }
   ],
   "source": [
    "python_new_4 = get_data_after('python', 'new', 't3_15yu4if')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2bdc231-aeb7-492b-91eb-ea88f300684e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15wcrev\n"
     ]
    }
   ],
   "source": [
    "python_new_5 = get_data_after('python', 'new', 't3_15xjs95')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a16d4904-c06d-4376-8ffc-dbfab7f3b65b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15ulfrw\n"
     ]
    }
   ],
   "source": [
    "python_new_6 = get_data_after('python', 'new', 't3_15wcrev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35859ce6-5dfd-47c0-badc-2f2a00a070e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15tnu99\n"
     ]
    }
   ],
   "source": [
    "python_new_7 = get_data_after('python', 'new', 't3_15ulfrw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dba998fa-df46-4db7-9060-90797b64033f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15s2lxn\n"
     ]
    }
   ],
   "source": [
    "python_new_8 = get_data_after('python', 'new', 't3_15tnu99')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5c61cec-75cc-484e-856d-0e9f294a8b14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15qq0ws\n"
     ]
    }
   ],
   "source": [
    "python_new_9 = get_data_after('python', 'new', 't3_15s2lxn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ba1e0fa-dded-4ea8-ba82-e8f19d23bcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15pfuyl\n"
     ]
    }
   ],
   "source": [
    "python_new_10 = get_data_after('python', 'new', 't3_15qq0ws')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b28c1991-dc16-4ae9-9585-6eba4544f68e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15o5h7b\n"
     ]
    }
   ],
   "source": [
    "python_new_11 = get_data_after('python', 'new', 't3_15pfuyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "431f3553-76bf-4161-b04a-e2d510ec82f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15mnhgo\n"
     ]
    }
   ],
   "source": [
    "python_new_12 = get_data_after('python', 'new', 't3_15o5h7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "698c9ff3-d174-444c-b31e-579461c37f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15lkt96\n"
     ]
    }
   ],
   "source": [
    "python_new_13 = get_data_after('python', 'new', 't3_15mnhgo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "285184c9-6801-4c77-8891-233890331498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15k0l15\n"
     ]
    }
   ],
   "source": [
    "python_new_14 = get_data_after('python', 'new', 't3_15lkt96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17aafe65-127e-452a-9415-505f477fe0e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15ixoqd\n"
     ]
    }
   ],
   "source": [
    "python_new_15 = get_data_after('python', 'new', 't3_15k0l15')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9a43d-a023-42d5-b28b-4317625db574",
   "metadata": {},
   "source": [
    "### Concatenate the data from all python subreddit web-scrapes into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "877b988c-1a5e-4a76-a6f9-b0e5d2d51355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_python = pd.concat([\n",
    "    python_hot_1, \n",
    "    python_hot_2, \n",
    "    python_hot_3, \n",
    "    python_hot_4, \n",
    "    python_hot_5, \n",
    "    python_hot_6, \n",
    "    python_hot_7, \n",
    "    python_hot_8, \n",
    "    python_hot_9, \n",
    "    python_hot_10,\n",
    "    python_hot_11,\n",
    "    python_hot_12,\n",
    "    python_hot_13,\n",
    "    python_hot_14,\n",
    "    python_hot_15,\n",
    "    \n",
    "    python_new_1,\n",
    "    python_new_2,\n",
    "    python_new_3,\n",
    "    python_new_4,\n",
    "    python_new_5,\n",
    "    python_new_6,\n",
    "    python_new_7,\n",
    "    python_new_8,\n",
    "    python_new_9,\n",
    "    python_new_10,\n",
    "    python_new_11,\n",
    "    python_new_12,\n",
    "    python_new_13,\n",
    "    python_new_14,\n",
    "    python_new_15\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14c0a480-e9f5-4ad5-aecb-dcd522898282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 6)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_python.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7969cf8c-1faf-413c-94dd-f5f1f4707789",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.693008e+09</td>\n",
       "      <td>Python</td>\n",
       "      <td>Saturday Daily Thread: Resource Request and Sh...</td>\n",
       "      <td>Found a neat resource related to Python over t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.693044e+09</td>\n",
       "      <td>Python</td>\n",
       "      <td>Understanding Immortal Objects in Python 3.12:...</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693095e+09</td>\n",
       "      <td>Python</td>\n",
       "      <td>Inference Llama 2 in one file of pure Python w...</td>\n",
       "      <td>Hi everyone,\\n\\nHave you ever wondering how to...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.693062e+09</td>\n",
       "      <td>Python</td>\n",
       "      <td>FastAPI + HTMX hello world demo app</td>\n",
       "      <td>Hello world!\\n\\nAs an effort in open source co...</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.693058e+09</td>\n",
       "      <td>Python</td>\n",
       "      <td>Robyn crosses 1M installs on PyPi</td>\n",
       "      <td>For the unaware, Robyn is a fast, async Python...</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc subreddit                                              title  \\\n",
       "0  1.693008e+09    Python  Saturday Daily Thread: Resource Request and Sh...   \n",
       "1  1.693044e+09    Python  Understanding Immortal Objects in Python 3.12:...   \n",
       "2  1.693095e+09    Python  Inference Llama 2 in one file of pure Python w...   \n",
       "3  1.693062e+09    Python                FastAPI + HTMX hello world demo app   \n",
       "4  1.693058e+09    Python                  Robyn crosses 1M installs on PyPi   \n",
       "\n",
       "                                                text  num_comments  score  \n",
       "0  Found a neat resource related to Python over t...             1      1  \n",
       "1                                                               10    127  \n",
       "2  Hi everyone,\\n\\nHave you ever wondering how to...             0      7  \n",
       "3  Hello world!\\n\\nAs an effort in open source co...             6     19  \n",
       "4  For the unaware, Robyn is a fast, async Python...             2     20  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_python.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbc09284-5d15-4efb-94f4-80c8e383a9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_python['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a44cf53-c740-4cfc-ad02-1acced0d4167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_python['text'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f635f-9b48-4468-abbd-5cc0f32c49bf",
   "metadata": {},
   "source": [
    "## Subreddit 2: Javascript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367586f-3f0d-45a4-8cdb-049a036bdbaf",
   "metadata": {},
   "source": [
    "### Webscrape data from the **javascript** and **java** subreddits and combine all requests into one DataFrame.\n",
    "##### **Javascript** subreddit did not have enough posts to equate to amount obtained from the python subreddit. Therefore, I included the **java** subreddit as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4330d0c4-bd09-415a-bc5c-c7df7187a227",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_1468vnz\n"
     ]
    }
   ],
   "source": [
    "java_hot_1 = get_data_initial('javascript', 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "633fcac5-e344-4246-91bd-912985b38059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_144d2bb\n"
     ]
    }
   ],
   "source": [
    "java_hot_2 = get_data_after('javascript', 'hot', 't3_1468vnz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11563ad8-3a69-4cd6-8711-efd417545d33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_14291sm\n"
     ]
    }
   ],
   "source": [
    "java_hot_3 = get_data_after('javascript', 'hot', 't3_144d2bb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "757f1387-dc2c-4242-b74f-291152f98e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13yhf7v\n"
     ]
    }
   ],
   "source": [
    "java_hot_4 = get_data_after('javascript', 'hot', 't3_14291sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77107d6c-2677-4c6f-809a-ea0b96525d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13uwv1a\n"
     ]
    }
   ],
   "source": [
    "java_hot_5 = get_data_after('javascript', 'hot', 't3_13yhf7v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f209bf17-9dc2-416d-aa88-4de12f246936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13sizi3\n"
     ]
    }
   ],
   "source": [
    "java_hot_6 = get_data_after('javascript', 'hot', 't3_13uwv1a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7191030-1b14-4ec4-86f8-6221c007a0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13r5zh7\n"
     ]
    }
   ],
   "source": [
    "java_hot_7 = get_data_after('javascript', 'hot', 't3_13sizi3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8ed075b-b902-4cfd-868b-35279ca4da09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13mqzkg\n"
     ]
    }
   ],
   "source": [
    "java_hot_8 = get_data_after('javascript', 'hot', 't3_13r5zh7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ddae760-d1dc-41bb-ad6c-d43237195c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13kslkc\n"
     ]
    }
   ],
   "source": [
    "java_hot_9 = get_data_after('javascript', 'hot', 't3_13mqzkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d91417f0-2e79-45be-bf7b-9883a1348057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13i9k74\n"
     ]
    }
   ],
   "source": [
    "java_hot_10 = get_data_after('javascript', 'hot', 't3_13kslkc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a94cbc1a-123f-40c3-83b0-e7ffaa8e559b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13eodzb\n"
     ]
    }
   ],
   "source": [
    "java_hot_11 = get_data_after('javascript', 'hot', 't3_13i9k74')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f3cdd5b-fdae-4c6f-a8ef-43736ab6f25a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13djtvp\n"
     ]
    }
   ],
   "source": [
    "java_hot_12 = get_data_after('javascript', 'hot', 't3_13eodzb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d17098af-cd51-486f-a65d-a3c6cb781734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_137v6f2\n"
     ]
    }
   ],
   "source": [
    "java_hot_13 = get_data_after('javascript', 'hot', 't3_13djtvp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8de9b0b3-ea8d-4361-bb8c-c45a8e7c30d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_136h57w\n"
     ]
    }
   ],
   "source": [
    "java_hot_14 = get_data_after('javascript', 'hot', 't3_137v6f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e67f5f6-254e-4d30-a0d5-dd5995b6ad1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_1347fui\n"
     ]
    }
   ],
   "source": [
    "java_hot_15 = get_data_after('javascript', 'hot', 't3_136h57w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe36dde6-39ee-4b74-9130-45dbe3a124f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_130k7et\n"
     ]
    }
   ],
   "source": [
    "java_hot_16 = get_data_after('javascript', 'hot', 't3_1347fui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2482a211-9ffe-4e2b-9191-8de36ab2b702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_12u4h8h\n"
     ]
    }
   ],
   "source": [
    "java_hot_17 = get_data_after('javascript', 'hot', 't3_130k7et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "169fe174-5d33-4ceb-b7b8-e1b4cd24e23b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_11tsto7\n"
     ]
    }
   ],
   "source": [
    "java_hot_18 = get_data_after('javascript', 'hot', 't3_12u4h8h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2fb6f88c-6cf1-4fea-9eee-d2295d739758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15wf0a0\n"
     ]
    }
   ],
   "source": [
    "java_hot_19 = get_data_initial('java', 'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27751317-26b0-4703-a452-95109a4c75e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15oed3x\n"
     ]
    }
   ],
   "source": [
    "java_hot_20 = get_data_after('java', 'hot', 't3_15wf0a0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07e029c4-ff59-4591-8bbd-2b582c2afe28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15hxp2f\n"
     ]
    }
   ],
   "source": [
    "java_hot_21 = get_data_after('java', 'hot', 't3_15oed3x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f6c950d-90d8-4b6b-8a78-b91a3b7a8b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_15ca0hn\n"
     ]
    }
   ],
   "source": [
    "java_hot_22 = get_data_after('java', 'hot', 't3_15hxp2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c984d86b-e0bf-489c-8ea6-1a8435e00de5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_154x68l\n"
     ]
    }
   ],
   "source": [
    "java_hot_23 = get_data_after('java', 'hot', 't3_15ca0hn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf634d99-a575-4825-9e31-0dd0b5935f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_14yhzm5\n"
     ]
    }
   ],
   "source": [
    "java_hot_24 = get_data_after('java', 'hot', 't3_154x68l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8858b1d-61bc-4a5a-b0fd-e6b2ead9c674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_14qissk\n"
     ]
    }
   ],
   "source": [
    "java_hot_25 = get_data_after('java', 'hot', 't3_14yhzm5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75551e81-97c5-4fdc-b1af-a0293a7a6d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_14kixyk\n"
     ]
    }
   ],
   "source": [
    "java_hot_26 = get_data_after('java', 'hot', 't3_14qissk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6133d3bd-b34e-42ad-a02f-8e9b10f7537b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_14f4xst\n"
     ]
    }
   ],
   "source": [
    "java_hot_27 = get_data_after('java', 'hot', 't3_14kixyk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dae16fa2-b0c2-4174-ba78-1d175fdde8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_143mqtz\n"
     ]
    }
   ],
   "source": [
    "java_hot_28 = get_data_after('java', 'hot', 't3_14f4xst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec428ef3-a733-405d-91be-b064c9496e10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13wxcti\n"
     ]
    }
   ],
   "source": [
    "java_hot_29 = get_data_after('java', 'hot', 't3_143mqtz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c74c7632-341a-43e9-93c2-c2a4c9567a02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_13mp9g0\n"
     ]
    }
   ],
   "source": [
    "java_hot_30 = get_data_after('java', 'hot', 't3_13wxcti')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2646b0-8356-48cb-b5ac-cd1bd83f2d72",
   "metadata": {},
   "source": [
    "### Concatenate the data from all javascript subreddit web-scrapes into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f62edab3-a065-4f32-8d76-32e64349204b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_java = pd.concat([\n",
    "    java_hot_1,\n",
    "    java_hot_2,\n",
    "    java_hot_3,\n",
    "    java_hot_4,\n",
    "    java_hot_5,\n",
    "    java_hot_6,\n",
    "    java_hot_7,\n",
    "    java_hot_8,\n",
    "    java_hot_9,\n",
    "    java_hot_10,\n",
    "    java_hot_11,\n",
    "    java_hot_12,\n",
    "    java_hot_13,\n",
    "    java_hot_14,\n",
    "    java_hot_15,\n",
    "    java_hot_16,\n",
    "    java_hot_17,\n",
    "    java_hot_18,\n",
    "    java_hot_19,\n",
    "    java_hot_20,\n",
    "    java_hot_21,\n",
    "    java_hot_22,\n",
    "    java_hot_23,\n",
    "    java_hot_24,\n",
    "    java_hot_25,\n",
    "    java_hot_26,\n",
    "    java_hot_27,\n",
    "    java_hot_28,\n",
    "    java_hot_29,\n",
    "    java_hot_30\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ca83e908-760d-4c09-b2a8-023927132b5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(710, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_java.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86062dc1-3e26-41a5-9499-108bd0f176e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.692609e+09</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Your /r/javascript recap for the week of Augus...</td>\n",
       "      <td>**Monday, August 14 - Sunday, August 20**\\n\\n#...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.692774e+09</td>\n",
       "      <td>javascript</td>\n",
       "      <td>WTF Wednesday (August 23, 2023)</td>\n",
       "      <td>Post a link to a GitHub repo or another code c...</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.692428e+09</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Showoff Saturday (August 19, 2023)</td>\n",
       "      <td>Did you find or create something cool this wee...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.692169e+09</td>\n",
       "      <td>javascript</td>\n",
       "      <td>WTF Wednesday (August 16, 2023)</td>\n",
       "      <td>Post a link to a GitHub repo or another code c...</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.692004e+09</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Your /r/javascript recap for the week of Augus...</td>\n",
       "      <td>**Monday, August 07 - Sunday, August 13**\\n\\n#...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc   subreddit  \\\n",
       "0  1.692609e+09  javascript   \n",
       "1  1.692774e+09  javascript   \n",
       "2  1.692428e+09  javascript   \n",
       "3  1.692169e+09  javascript   \n",
       "4  1.692004e+09  javascript   \n",
       "\n",
       "                                               title  \\\n",
       "0  Your /r/javascript recap for the week of Augus...   \n",
       "1                    WTF Wednesday (August 23, 2023)   \n",
       "2                 Showoff Saturday (August 19, 2023)   \n",
       "3                    WTF Wednesday (August 16, 2023)   \n",
       "4  Your /r/javascript recap for the week of Augus...   \n",
       "\n",
       "                                                text  num_comments  score  \n",
       "0  **Monday, August 14 - Sunday, August 20**\\n\\n#...             4      0  \n",
       "1  Post a link to a GitHub repo or another code c...             6     23  \n",
       "2  Did you find or create something cool this wee...            22     22  \n",
       "3  Post a link to a GitHub repo or another code c...            18     39  \n",
       "4  **Monday, August 07 - Sunday, August 13**\\n\\n#...             1     21  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_java.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87a8c2-3cf9-4a3a-ba6d-ffab40c69af8",
   "metadata": {},
   "source": [
    "## Concatenate the Python and Java DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "841091fe-c409-4d48-867f-ab233edfbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_programming = pd.concat([df_python, df_java])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634af806-ce51-48d3-bdd0-69b02302dec8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1cb3a-3031-425a-82c8-47f6c7ab6ad5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create functions to lemmatize and stem text and title of each post in both "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee6ea2d-979a-4f31-b219-a9f4bad17c67",
   "metadata": {},
   "source": [
    "### Function to lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e01bacd-30a1-471c-88aa-1044375123bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_lemmatizer(review):\n",
    "    \n",
    "    # Set token and instantiate Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    my_tokenizer = RegexpTokenizer(\"[\\w']+|\\$[\\d\\.]+\")   \n",
    "\n",
    "    # Tokenize words\n",
    "    words = my_tokenizer.tokenize(review.lower())\n",
    "    \n",
    "    # Remove stopwords and add any words that should be included\n",
    "    stop_word_list = stopwords.words('english')\n",
    "    \n",
    "    non_stop_words = [word for word in words if word not in stop_word_list]\n",
    "    \n",
    "    # Lemmatize\n",
    "    review_lem = [lemmatizer.lemmatize(word) for word in non_stop_words]\n",
    "    \n",
    "    \n",
    "    # Put words back together into a single string. \n",
    "    return ' '.join(review_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144e583c-50b5-4cef-879a-2eb8e01a466a",
   "metadata": {},
   "source": [
    "----\n",
    "### Add Lemmatized text to a new column in both DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa6c51e3-a55a-4c35-b5b5-57b1c4ee3b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_programming['lemm_text'] = df_programming['text'].map(text_lemmatizer)\n",
    "df_programming['lemm_title'] = df_programming['title'].map(text_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5a98095c-43c2-4121-9f00-eabbb0f5ff7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>lemm_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.693008e+09</td>\n",
       "      <td>Python</td>\n",
       "      <td>Saturday Daily Thread: Resource Request and Sh...</td>\n",
       "      <td>Found a neat resource related to Python over t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>found neat resource related past week looking ...</td>\n",
       "      <td>saturday daily thread resource request sharing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.693044e+09</td>\n",
       "      <td>Python</td>\n",
       "      <td>Understanding Immortal Objects in Python 3.12:...</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>127</td>\n",
       "      <td></td>\n",
       "      <td>understanding immortal object 3 12 deep dive i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    created_utc subreddit                                              title  \\\n",
       "0  1.693008e+09    Python  Saturday Daily Thread: Resource Request and Sh...   \n",
       "1  1.693044e+09    Python  Understanding Immortal Objects in Python 3.12:...   \n",
       "\n",
       "                                                text  num_comments  score  \\\n",
       "0  Found a neat resource related to Python over t...             1      1   \n",
       "1                                                               10    127   \n",
       "\n",
       "                                           lemm_text  \\\n",
       "0  found neat resource related past week looking ...   \n",
       "1                                                      \n",
       "\n",
       "                                          lemm_title  \n",
       "0  saturday daily thread resource request sharing...  \n",
       "1  understanding immortal object 3 12 deep dive i...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_programming.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8701d6fc-8eab-484f-921d-ef06dcea44ac",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101f8a8-4f53-4bf8-a309-fbf64b0d9d57",
   "metadata": {},
   "source": [
    "### Function to Stem the values in the text and title columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf04ca6-c45d-463a-a306-ad0d1a549a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_stem(review):\n",
    "    \n",
    "    # Set token and instantiate Stem\n",
    "    p_stem = PorterStemmer()\n",
    "    my_tokenizer = RegexpTokenizer(\"[\\w']+|\\$[\\d\\.]+\")   \n",
    "\n",
    "    # Tokenize words\n",
    "    words = my_tokenizer.tokenize(review.lower())\n",
    "    \n",
    "    # Remove stop words and add words to stop word list that should be included\n",
    "    stop_word_list = stopwords.words('english')\n",
    "    \n",
    "    non_stop_words = [word for word in words if word not in stop_word_list]\n",
    "    \n",
    "    # Stem\n",
    "    review_stem = [p_stem.stem(word) for word in non_stop_words]    \n",
    "    \n",
    "    # Put words back together into a single string. It is easier to work with a string than a list for NLP.\n",
    "    return ' '.join(review_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d5a5b-f3cc-4593-a312-1dd8860bc365",
   "metadata": {},
   "source": [
    "----\n",
    "### Add Stemmed text to a new column in each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10eebe73-a604-4b64-b457-14f19cf9ac45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_programming['stem_text'] = df_programming['text'].map(text_stem)\n",
    "df_programming['stem_title'] = df_programming['title'].map(text_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f090d-f7d1-47ce-8c27-010bafccd2aa",
   "metadata": {},
   "source": [
    "---\n",
    "### Remove columns that won't be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7855eb3a-a990-49c3-ae13-eefd8ecda8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_programming.drop(columns = ['created_utc', 'title', 'text', 'num_comments', 'score'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c38e853b-a5fa-42d6-9e40-6960890481aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>lemm_title</th>\n",
       "      <th>stem_text</th>\n",
       "      <th>stem_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python</td>\n",
       "      <td>found neat resource related past week looking ...</td>\n",
       "      <td>saturday daily thread resource request sharing...</td>\n",
       "      <td>found neat resourc relat past week look resour...</td>\n",
       "      <td>saturday daili thread resourc request share da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td></td>\n",
       "      <td>understanding immortal object 3 12 deep dive i...</td>\n",
       "      <td></td>\n",
       "      <td>understand immort object 3 12 deep dive intern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                          lemm_text  \\\n",
       "0    Python  found neat resource related past week looking ...   \n",
       "1    Python                                                      \n",
       "\n",
       "                                          lemm_title  \\\n",
       "0  saturday daily thread resource request sharing...   \n",
       "1  understanding immortal object 3 12 deep dive i...   \n",
       "\n",
       "                                           stem_text  \\\n",
       "0  found neat resourc relat past week look resour...   \n",
       "1                                                      \n",
       "\n",
       "                                          stem_title  \n",
       "0  saturday daili thread resourc request share da...  \n",
       "1     understand immort object 3 12 deep dive intern  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_programming.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f11fadf-731c-4c38-b493-34d5da643477",
   "metadata": {},
   "source": [
    "---\n",
    "### Encode the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6984f3d2-6927-4cb4-afb3-63fc5937552a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_programming.insert(loc = 1,\n",
    "                      column = 'target',\n",
    "                      value = np.where(df_programming['subreddit'] == 'Python', 1, 0)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b6ba966d-51a5-4a98-927a-9b4348914e67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.50419\n",
       "0    0.49581\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_programming['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d78aa-6077-443b-92e4-8b64557d5aaf",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d6dc66-2589-4825-88a7-5e52177e0431",
   "metadata": {},
   "source": [
    "#### Note that there are some observations with missing values. To alleviate, this I decided to copy the lemmatized text to the missing values in the lemmatize title feature of the same observation and vice versa. The same will be done for stemmed text and stemmed title.\n",
    "\n",
    "#### Creating a new DataFrame that only contains lemm_text, lemm_title, stem_text and stem_title, which will be the datase used for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9c8cfdf4-9269-4a18-ab05-2b84dde641fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>target</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>lemm_title</th>\n",
       "      <th>stem_text</th>\n",
       "      <th>stem_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>found neat resource related past week looking ...</td>\n",
       "      <td>saturday daily thread resource request sharing...</td>\n",
       "      <td>found neat resourc relat past week look resour...</td>\n",
       "      <td>saturday daili thread resourc request share da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>understanding immortal object 3 12 deep dive i...</td>\n",
       "      <td></td>\n",
       "      <td>understand immort object 3 12 deep dive intern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone ever wondering implement complex s...</td>\n",
       "      <td>inference llama 2 one file pure zero dependency</td>\n",
       "      <td>hi everyon ever wonder implement complex scien...</td>\n",
       "      <td>infer llama 2 one file pure zero depend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>hello world effort open source contribution ma...</td>\n",
       "      <td>fastapi htmx hello world demo app</td>\n",
       "      <td>hello world effort open sourc contribut made f...</td>\n",
       "      <td>fastapi htmx hello world demo app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>unaware robyn fast async web framework rust ru...</td>\n",
       "      <td>robyn cross 1m installs pypi</td>\n",
       "      <td>unawar robyn fast async web framework rust run...</td>\n",
       "      <td>robyn cross 1m instal pypi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  target                                          lemm_text  \\\n",
       "0    Python       1  found neat resource related past week looking ...   \n",
       "1    Python       1                                                      \n",
       "2    Python       1  hi everyone ever wondering implement complex s...   \n",
       "3    Python       1  hello world effort open source contribution ma...   \n",
       "4    Python       1  unaware robyn fast async web framework rust ru...   \n",
       "\n",
       "                                          lemm_title  \\\n",
       "0  saturday daily thread resource request sharing...   \n",
       "1  understanding immortal object 3 12 deep dive i...   \n",
       "2    inference llama 2 one file pure zero dependency   \n",
       "3                  fastapi htmx hello world demo app   \n",
       "4                       robyn cross 1m installs pypi   \n",
       "\n",
       "                                           stem_text  \\\n",
       "0  found neat resourc relat past week look resour...   \n",
       "1                                                      \n",
       "2  hi everyon ever wonder implement complex scien...   \n",
       "3  hello world effort open sourc contribut made f...   \n",
       "4  unawar robyn fast async web framework rust run...   \n",
       "\n",
       "                                          stem_title  \n",
       "0  saturday daili thread resourc request share da...  \n",
       "1     understand immort object 3 12 deep dive intern  \n",
       "2            infer llama 2 one file pure zero depend  \n",
       "3                  fastapi htmx hello world demo app  \n",
       "4                         robyn cross 1m instal pypi  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_programming.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e232ae1-1b1c-4017-988b-af4d942af5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_programming_filled = df_programming.copy()\n",
    "\n",
    "df_programming_filled['lemm_text'] = np.where(df_programming_filled['lemm_text'] == '', df_programming_filled['lemm_title'], df_programming_filled['lemm_text'])\n",
    "\n",
    "df_programming_filled['lemm_title'] = np.where(df_programming_filled['lemm_title'] == '', df_programming_filled['lemm_text'], df_programming_filled['lemm_title'])\n",
    "\n",
    "df_programming_filled['stem_text'] = np.where(df_programming_filled['stem_text'] == '', df_programming_filled['stem_title'], df_programming_filled['stem_text'])\n",
    "\n",
    "df_programming_filled['stem_title'] = np.where(df_programming_filled['stem_title'] == '', df_programming_filled['stem_text'], df_programming_filled['stem_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "708d0ff1-6f0e-4e19-8e93-25d2c6482af2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>target</th>\n",
       "      <th>lemm_text</th>\n",
       "      <th>lemm_title</th>\n",
       "      <th>stem_text</th>\n",
       "      <th>stem_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>found neat resource related past week looking ...</td>\n",
       "      <td>saturday daily thread resource request sharing...</td>\n",
       "      <td>found neat resourc relat past week look resour...</td>\n",
       "      <td>saturday daili thread resourc request share da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>understanding immortal object 3 12 deep dive i...</td>\n",
       "      <td>understanding immortal object 3 12 deep dive i...</td>\n",
       "      <td>understand immort object 3 12 deep dive intern</td>\n",
       "      <td>understand immort object 3 12 deep dive intern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>hi everyone ever wondering implement complex s...</td>\n",
       "      <td>inference llama 2 one file pure zero dependency</td>\n",
       "      <td>hi everyon ever wonder implement complex scien...</td>\n",
       "      <td>infer llama 2 one file pure zero depend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>hello world effort open source contribution ma...</td>\n",
       "      <td>fastapi htmx hello world demo app</td>\n",
       "      <td>hello world effort open sourc contribut made f...</td>\n",
       "      <td>fastapi htmx hello world demo app</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Python</td>\n",
       "      <td>1</td>\n",
       "      <td>unaware robyn fast async web framework rust ru...</td>\n",
       "      <td>robyn cross 1m installs pypi</td>\n",
       "      <td>unawar robyn fast async web framework rust run...</td>\n",
       "      <td>robyn cross 1m instal pypi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  target                                          lemm_text  \\\n",
       "0    Python       1  found neat resource related past week looking ...   \n",
       "1    Python       1  understanding immortal object 3 12 deep dive i...   \n",
       "2    Python       1  hi everyone ever wondering implement complex s...   \n",
       "3    Python       1  hello world effort open source contribution ma...   \n",
       "4    Python       1  unaware robyn fast async web framework rust ru...   \n",
       "\n",
       "                                          lemm_title  \\\n",
       "0  saturday daily thread resource request sharing...   \n",
       "1  understanding immortal object 3 12 deep dive i...   \n",
       "2    inference llama 2 one file pure zero dependency   \n",
       "3                  fastapi htmx hello world demo app   \n",
       "4                       robyn cross 1m installs pypi   \n",
       "\n",
       "                                           stem_text  \\\n",
       "0  found neat resourc relat past week look resour...   \n",
       "1     understand immort object 3 12 deep dive intern   \n",
       "2  hi everyon ever wonder implement complex scien...   \n",
       "3  hello world effort open sourc contribut made f...   \n",
       "4  unawar robyn fast async web framework rust run...   \n",
       "\n",
       "                                          stem_title  \n",
       "0  saturday daili thread resourc request share da...  \n",
       "1     understand immort object 3 12 deep dive intern  \n",
       "2            infer llama 2 one file pure zero depend  \n",
       "3                  fastapi htmx hello world demo app  \n",
       "4                         robyn cross 1m instal pypi  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_programming_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f892f38-e764-4e34-9f85-625552441512",
   "metadata": {},
   "source": [
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ffbbb-5173-4080-a6d5-ad2c0d2504e6",
   "metadata": {},
   "source": [
    "### Export the final DataFrame as a csv and use it to begin modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bcbeb0ab-889e-42b1-80f9-e340864ea9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_programming_filled.to_csv('./P4_Datasets/programming_languages')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
